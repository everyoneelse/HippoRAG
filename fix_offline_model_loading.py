#!/usr/bin/env python3
"""
ä¿®å¤æœ¬åœ°æ¨¡å‹åŠ è½½æ—¶çš„ç½‘ç»œè¿æ¥é—®é¢˜
"""

import os
import argparse

def set_offline_environment():
    """è®¾ç½®ç¦»çº¿ç¯å¢ƒå˜é‡"""
    print("ğŸ”§ è®¾ç½®ç¦»çº¿ç¯å¢ƒå˜é‡...")
    
    # è®¾ç½® Hugging Face ç¦»çº¿æ¨¡å¼
    os.environ['HF_HUB_OFFLINE'] = '1'
    os.environ['TRANSFORMERS_OFFLINE'] = '1'
    
    # ç¦ç”¨ç½‘ç»œæ£€æŸ¥
    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
    os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
    
    print("âœ… å·²è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
    print("   HF_HUB_OFFLINE=1")
    print("   TRANSFORMERS_OFFLINE=1") 
    print("   HF_HUB_DISABLE_TELEMETRY=1")
    print("   HF_HUB_DISABLE_PROGRESS_BARS=1")

def check_local_model_files(model_path):
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´"""
    print(f"\nğŸ” æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {model_path}")
    
    required_files = [
        'config.json',
        'tokenizer_config.json',
        'tokenizer.json',
        'vocab.txt',
        'special_tokens_map.json'
    ]
    
    model_files = [
        'pytorch_model.bin',
        'model.safetensors',
        'pytorch_model-00001-of-00001.bin'
    ]
    
    missing_files = []
    found_model_file = False
    
    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file}")
        else:
            print(f"âŒ {file} (ç¼ºå¤±)")
            missing_files.append(file)
    
    # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆè‡³å°‘éœ€è¦ä¸€ä¸ªï¼‰
    for file in model_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file}")
            found_model_file = True
            break
    
    if not found_model_file:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ (pytorch_model.bin æˆ– model.safetensors)")
        missing_files.append("æ¨¡å‹æƒé‡æ–‡ä»¶")
    
    if missing_files:
        print(f"\nâš ï¸  ç¼ºå¤±æ–‡ä»¶: {', '.join(missing_files)}")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. é‡æ–°ä¸‹è½½å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶")
        print("   2. æˆ–è€…å…è®¸ç½‘ç»œè®¿é—®ä»¥è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±æ–‡ä»¶")
        return False
    else:
        print("\nâœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨")
        return True

def create_offline_demo():
    """åˆ›å»ºç¦»çº¿ä½¿ç”¨ç¤ºä¾‹"""
    demo_content = '''#!/usr/bin/env python3
"""
ç¦»çº¿ä½¿ç”¨ HippoRAG çš„ç¤ºä¾‹è„šæœ¬
"""

import os

# è®¾ç½®ç¦»çº¿ç¯å¢ƒå˜é‡
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

from src.hipporag.HippoRAG import HippoRAG
from src.hipporag.utils.config_utils import BaseConfig

def main():
    print("ğŸš€ ç¦»çº¿æ¨¡å¼ HippoRAG æ¼”ç¤º")
    
    # é…ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„
    local_model_path = "/mnt/data/hy/GPT/models/nvidia/NV-Embed-v2"
    
    config = BaseConfig(
        save_dir='offline_outputs',
        llm_name='gpt-4o-mini',  # æˆ–ä½¿ç”¨æœ¬åœ° LLM
        embedding_model_name=local_model_path,
        dataset='offline_demo',
        force_index_from_scratch=True
    )
    
    # åˆå§‹åŒ– HippoRAG
    hipporag = HippoRAG(global_config=config)
    
    # ç¤ºä¾‹æ–‡æ¡£
    docs = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚",
        "æ·±åº¦å­¦ä¹ åŸºäºäººå·¥ç¥ç»ç½‘ç»œã€‚"
    ]
    
    # æ„å»ºçŸ¥è¯†å›¾è°±
    print("ğŸ“š æ„å»ºçŸ¥è¯†å›¾è°±...")
    hipporag.index(docs)
    
    # å¯¼å‡ºçŸ¥è¯†å›¾è°±
    print("ğŸ’¾ å¯¼å‡ºçŸ¥è¯†å›¾è°±...")
    hipporag.export_knowledge_graph('json', 'offline_graph.json')
    
    print("âœ… ç¦»çº¿æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    main()
'''
    
    with open('offline_demo.py', 'w', encoding='utf-8') as f:
        f.write(demo_content)
    
    print("ğŸ“ å·²åˆ›å»ºç¦»çº¿æ¼”ç¤ºè„šæœ¬: offline_demo.py")

def main():
    parser = argparse.ArgumentParser(description="ä¿®å¤æœ¬åœ°æ¨¡å‹åŠ è½½é—®é¢˜")
    parser.add_argument('--model_path', type=str, 
                       default='/mnt/data/hy/GPT/models/nvidia/NV-Embed-v2',
                       help='æœ¬åœ°æ¨¡å‹è·¯å¾„')
    parser.add_argument('--check_files', action='store_true',
                       help='æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§')
    parser.add_argument('--set_offline', action='store_true',
                       help='è®¾ç½®ç¦»çº¿ç¯å¢ƒå˜é‡')
    parser.add_argument('--create_demo', action='store_true',
                       help='åˆ›å»ºç¦»çº¿æ¼”ç¤ºè„šæœ¬')
    
    args = parser.parse_args()
    
    print("ğŸ”§ HippoRAG æœ¬åœ°æ¨¡å‹åŠ è½½ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    if args.check_files:
        check_local_model_files(args.model_path)
    
    if args.set_offline:
        set_offline_environment()
    
    if args.create_demo:
        create_offline_demo()
    
    if not any([args.check_files, args.set_offline, args.create_demo]):
        # é»˜è®¤æ‰§è¡Œæ‰€æœ‰æ“ä½œ
        check_local_model_files(args.model_path)
        set_offline_environment()
        create_offline_demo()
    
    print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆæ€»ç»“:")
    print("1. åœ¨è¿è¡Œè„šæœ¬å‰è®¾ç½®ç¯å¢ƒå˜é‡:")
    print("   export HF_HUB_OFFLINE=1")
    print("   export TRANSFORMERS_OFFLINE=1")
    
    print(f"\n2. æˆ–è€…åœ¨ Python ä»£ç å¼€å¤´æ·»åŠ :")
    print("   import os")
    print("   os.environ['HF_HUB_OFFLINE'] = '1'")
    print("   os.environ['TRANSFORMERS_OFFLINE'] = '1'")
    
    print(f"\n3. ä½¿ç”¨ä¿®æ”¹åçš„è„šæœ¬:")
    print(f"   python export_knowledge_graph.py \\")
    print(f"     --embedding_path {args.model_path} \\")
    print(f"     --dataset sample")

if __name__ == "__main__":
    main()