# HippoRAGçŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼æ·±åº¦åˆ†æå¯¹è¯è®°å½•

## ç”¨æˆ·è¯¢é—®ï¼šé‡‡ç”¨äº†ä»€ä¹ˆæ ·çš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼

æˆ‘éœ€è¦å…ˆäº†è§£å½“å‰å·¥ä½œåŒºçš„å†…å®¹ï¼Œä»¥åˆ†æé‡‡ç”¨äº†ä»€ä¹ˆæ ·çš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼ã€‚è®©æˆ‘å…ˆæŸ¥çœ‹å·¥ä½œåŒºçš„ç»“æ„ã€‚

é€šè¿‡å¯¹ä»£ç çš„æ·±å…¥åˆ†æï¼ŒHippoRAG é‡‡ç”¨äº†ä»¥ä¸‹çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼ï¼š

## HippoRAG é‡‡ç”¨çš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼

### 1. **åŸºäºOpenIEçš„ä¸‰å…ƒç»„æŠ½å–**

HippoRAG ä½¿ç”¨å¼€æ”¾ä¿¡æ¯æŠ½å–ï¼ˆOpenIEï¼‰æŠ€æœ¯æ¥æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼š

- **å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰**ï¼šä»æ–‡æ¡£ä¸­è¯†åˆ«å’Œæå–å‘½åå®ä½“
- **ä¸‰å…ƒç»„æŠ½å–**ï¼šåŸºäºè¯†åˆ«çš„å®ä½“æ„å»ºRDFæ ¼å¼çš„ä¸‰å…ƒç»„å…³ç³»

```python
# ä»ä»£ç ä¸­å¯ä»¥çœ‹åˆ°NERå’Œä¸‰å…ƒç»„æŠ½å–çš„æµç¨‹
def openie(self, chunk_key: str, passage: str) -> Dict[str, Any]:
    ner_output = self.ner(chunk_key=chunk_key, passage=passage)
    triple_output = self.triple_extraction(chunk_key=chunk_key, passage=passage, 
                                         named_entities=ner_output.unique_entities)
```

### 2. **å¤šå±‚æ¬¡å›¾ç»“æ„**

æ„å»ºçš„çŸ¥è¯†å›¾è°±åŒ…å«ä¸‰ç§ç±»å‹çš„èŠ‚ç‚¹å’Œè¾¹ï¼š

#### **èŠ‚ç‚¹ç±»å‹ï¼š**
- **å®ä½“èŠ‚ç‚¹ï¼ˆEntity Nodesï¼‰**ï¼šä»ä¸‰å…ƒç»„ä¸­æå–çš„ä¸»è¯­å’Œå®¾è¯­å®ä½“
- **æ®µè½èŠ‚ç‚¹ï¼ˆPassage Nodesï¼‰**ï¼šåŸå§‹æ–‡æ¡£çš„æ–‡æœ¬æ®µè½
- **äº‹å®èŠ‚ç‚¹ï¼ˆFact Nodesï¼‰**ï¼šå®Œæ•´çš„ä¸‰å…ƒç»„å…³ç³»

#### **è¾¹ç±»å‹ï¼š**
- **äº‹å®è¾¹ï¼ˆFact Edgesï¼‰**ï¼šè¿æ¥ä¸‰å…ƒç»„ä¸­çš„ä¸»è¯­å’Œå®¾è¯­å®ä½“
- **æ®µè½-å®ä½“è¾¹ï¼ˆPassage-Entity Edgesï¼‰**ï¼šè¿æ¥æ®µè½å’Œå…¶åŒ…å«çš„å®ä½“
- **åŒä¹‰è¯è¾¹ï¼ˆSynonymy Edgesï¼‰**ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿æ¥ç›¸ä¼¼å®ä½“

### 3. **åŸºäºåµŒå…¥çš„åŒä¹‰è¯è¾¹æ„å»º**

HippoRAG ä½¿ç”¨å…ˆè¿›çš„åµŒå…¥æŠ€æœ¯æ¥è¯†åˆ«å’Œè¿æ¥è¯­ä¹‰ç›¸ä¼¼çš„å®ä½“ï¼š

```python
def add_synonymy_edges(self):
    # è·å–å®ä½“åµŒå…¥
    entity_embs = self.entity_embedding_store.get_embeddings(entity_node_keys)
    
    # KNNæ£€ç´¢æ‰¾åˆ°ç›¸ä¼¼å®ä½“
    query_node_key2knn_node_keys = retrieve_knn(
        query_ids=entity_node_keys,
        key_ids=entity_node_keys,
        query_vecs=entity_embs,
        key_vecs=entity_embs,
        k=self.global_config.synonymy_edge_topk
    )
```

### 4. **å¢é‡å¼å›¾æ„å»º**

æ”¯æŒå¢é‡æ›´æ–°å’Œåˆ é™¤æ“ä½œï¼Œå…è®¸åŠ¨æ€ç»´æŠ¤çŸ¥è¯†å›¾è°±ï¼š

- æ–°æ–‡æ¡£å¯ä»¥æ·»åŠ åˆ°ç°æœ‰å›¾è°±ä¸­
- æ”¯æŒåˆ é™¤ç‰¹å®šæ–‡æ¡£åŠå…¶ç›¸å…³å®ä½“å’Œå…³ç³»
- ä¿æŒå›¾è°±çš„ä¸€è‡´æ€§å’Œå®Œæ•´æ€§

### 5. **å¤šæ¨¡æ€åµŒå…¥å­˜å‚¨**

ä½¿ç”¨ä¸“é—¨çš„åµŒå…¥å­˜å‚¨ç³»ç»Ÿåˆ†åˆ«ç®¡ç†ï¼š
- **æ®µè½åµŒå…¥**ï¼šç”¨äºå¯†é›†æ®µè½æ£€ç´¢
- **å®ä½“åµŒå…¥**ï¼šç”¨äºå®ä½“é“¾æ¥å’ŒåŒä¹‰è¯å‘ç°
- **äº‹å®åµŒå…¥**ï¼šç”¨äºäº‹å®æ£€ç´¢å’Œç›¸å…³æ€§è¯„åˆ†

### 6. **é…ç½®åŒ–çš„å›¾æ„å»ºå‚æ•°**

æä¾›ä¸°å¯Œçš„é…ç½®é€‰é¡¹æ¥æ§åˆ¶å›¾æ„å»ºè¿‡ç¨‹ï¼š

```python
# åŒä¹‰è¯è¾¹æ„å»ºå‚æ•°
synonymy_edge_topk: int = 2047  # KNNæ£€ç´¢çš„Kå€¼
synonymy_edge_sim_threshold: float = 0.8  # ç›¸ä¼¼åº¦é˜ˆå€¼
synonymy_edge_query_batch_size: int = 1000  # æŸ¥è¯¢æ‰¹æ¬¡å¤§å°
synonymy_edge_key_batch_size: int = 10000  # é”®æ‰¹æ¬¡å¤§å°

# å›¾ç±»å‹é€‰æ‹©
graph_type: Literal[
    'dpr_only', 
    'entity', 
    'passage_entity', 
    'relation_aware_passage_entity',
    'passage_entity_relation', 
    'facts_and_sim_passage_node_unidirectional'
] = "facts_and_sim_passage_node_unidirectional"
```

### 7. **LLMé©±åŠ¨çš„æ™ºèƒ½æŠ½å–**

ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜è´¨é‡çš„ä¿¡æ¯æŠ½å–ï¼š

- **æç¤ºå·¥ç¨‹**ï¼šä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æç¤ºæ¨¡æ¿
- **Few-shotå­¦ä¹ **ï¼šé€šè¿‡ç¤ºä¾‹å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ­£ç¡®æ ¼å¼çš„è¾“å‡º
- **é”™è¯¯å¤„ç†**ï¼šåŒ…å«å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œç»“æœéªŒè¯æœºåˆ¶

### 8. **å›¾å­˜å‚¨å’ŒæŒä¹…åŒ–**

- ä½¿ç”¨iGraphåº“è¿›è¡Œå›¾æ•°æ®ç»“æ„ç®¡ç†
- æ”¯æŒå›¾çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼ˆPickleæ ¼å¼ï¼‰
- åˆ†ç¦»çš„åµŒå…¥å­˜å‚¨ç³»ç»Ÿç”¨äºé«˜æ•ˆçš„å‘é‡æ£€ç´¢

è¿™ç§çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼çš„ç‰¹ç‚¹æ˜¯ï¼š
- **è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜**ï¼šå®Œå…¨åŸºäºLLMè‡ªåŠ¨æŠ½å–ï¼Œæ— éœ€äººå·¥æ ‡æ³¨
- **è¯­ä¹‰ä¸°å¯Œ**ï¼šç»“åˆäº†ç¬¦å·çŸ¥è¯†ï¼ˆä¸‰å…ƒç»„ï¼‰å’Œå‘é‡çŸ¥è¯†ï¼ˆåµŒå…¥ï¼‰
- **å¯æ‰©å±•æ€§å¼º**ï¼šæ”¯æŒå¢é‡æ›´æ–°å’Œå¤§è§„æ¨¡æ–‡æ¡£å¤„ç†
- **æ£€ç´¢å‹å¥½**ï¼šä¸“ä¸ºRAGä»»åŠ¡ä¼˜åŒ–ï¼Œæ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥

## ç”¨æˆ·è¯¢é—®ï¼šLLMé©±åŠ¨çš„æ™ºèƒ½æŠ½å–è¯¦ç»†å±•å¼€

åŸºäºä»£ç çš„æ·±å…¥åˆ†æï¼Œæˆ‘æ¥è¯¦ç»†å±•å¼€HippoRAGä¸­LLMé©±åŠ¨çš„æ™ºèƒ½æŠ½å–æœºåˆ¶ï¼š

## HippoRAGä¸­LLMé©±åŠ¨çš„æ™ºèƒ½æŠ½å–è¯¦ç»†æœºåˆ¶

### 1. **ä¸¤é˜¶æ®µæŠ½å–æµç¨‹**

HippoRAGé‡‡ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„ä¸¤é˜¶æ®µä¿¡æ¯æŠ½å–æµç¨‹ï¼š

#### **é˜¶æ®µä¸€ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰**
```python
def ner(self, chunk_key: str, passage: str) -> NerRawOutput:
    # ä½¿ç”¨ä¸“é—¨çš„NERæç¤ºæ¨¡æ¿
    ner_input_message = self.prompt_template_manager.render(name='ner', passage=passage)
    
    # LLMæ¨ç†
    raw_response, metadata, cache_hit = self.llm_model.infer(messages=ner_input_message)
    
    # è§£æJSONå“åº”
    extracted_entities = _extract_ner_from_response(real_response)
    unique_entities = list(dict.fromkeys(extracted_entities))
```

**NERæç¤ºæ¨¡æ¿ç‰¹ç‚¹ï¼š**
- **ç³»ç»ŸæŒ‡ä»¤æ˜ç¡®**ï¼š`"Your task is to extract named entities from the given paragraph. Respond with a JSON list of entities."`
- **Few-shotå­¦ä¹ **ï¼šæä¾›å…·ä½“ç¤ºä¾‹ï¼ˆRadio Cityæ¡ˆä¾‹ï¼‰
- **ç»“æ„åŒ–è¾“å‡º**ï¼šè¦æ±‚JSONæ ¼å¼ï¼Œä¾¿äºåç»­å¤„ç†

#### **é˜¶æ®µäºŒï¼šå…³ç³»ä¸‰å…ƒç»„æŠ½å–**
```python
def triple_extraction(self, chunk_key: str, passage: str, named_entities: List[str]) -> TripleRawOutput:
    # åŸºäºNERç»“æœæ„å»ºä¸‰å…ƒç»„æŠ½å–æç¤º
    messages = self.prompt_template_manager.render(
        name='triple_extraction',
        passage=passage,
        named_entity_json=json.dumps({"named_entities": named_entities})
    )
```

**ä¸‰å…ƒç»„æŠ½å–çš„æ™ºèƒ½ç‰¹æ€§ï¼š**
- **æ¡ä»¶åŒ–æŠ½å–**ï¼šåŸºäºç¬¬ä¸€é˜¶æ®µçš„NERç»“æœè¿›è¡Œä¸‰å…ƒç»„æŠ½å–
- **RDFè§„èŒƒ**ï¼šæ„å»ºç¬¦åˆRDFæ ‡å‡†çš„çŸ¥è¯†å›¾è°±
- **å®ä½“çº¦æŸ**ï¼šè¦æ±‚ä¸‰å…ƒç»„åŒ…å«è‡³å°‘ä¸€ä¸ªå·²è¯†åˆ«çš„å‘½åå®ä½“
- **æŒ‡ä»£æ¶ˆè§£**ï¼šæ˜ç¡®è¦æ±‚"Clearly resolve pronouns to their specific names"

### 2. **ç²¾å¿ƒè®¾è®¡çš„æç¤ºå·¥ç¨‹**

#### **NERæç¤ºç¤ºä¾‹ï¼š**
```python
ner_system = """Your task is to extract named entities from the given paragraph. 
Respond with a JSON list of entities."""

# Few-shotç¤ºä¾‹
one_shot_ner_paragraph = """Radio City
Radio City is India's first private FM radio station and was started on 3 July 2001.
It plays Hindi, English and regional songs..."""

one_shot_ner_output = """{"named_entities":
    ["Radio City", "India", "3 July 2001", "Hindi", "English", "May 2008", "PlanetRadiocity.com"]
}"""
```

#### **ä¸‰å…ƒç»„æŠ½å–æç¤ºç¤ºä¾‹ï¼š**
```python
ner_conditioned_re_system = """Your task is to construct an RDF (Resource Description Framework) graph from the given passages and named entity lists. 
Respond with a JSON list of triples, with each triple representing a relationship in the RDF graph. 

Pay attention to the following requirements:
- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.
- Clearly resolve pronouns to their specific names to maintain clarity."""

# å¯¹åº”çš„è¾“å‡ºç¤ºä¾‹
ner_conditioned_re_output = """{"triples": [
    ["Radio City", "located in", "India"],
    ["Radio City", "is", "private FM radio station"],
    ["Radio City", "started on", "3 July 2001"],
    ["PlanetRadiocity.com", "launched in", "May 2008"],
    ["PlanetRadiocity.com", "is", "music portal"]
]}"""
```

### 3. **æ™ºèƒ½é”™è¯¯å¤„ç†å’Œä¿®å¤æœºåˆ¶**

#### **JSONä¿®å¤åŠŸèƒ½ï¼š**
```python
def fix_broken_generated_json(json_str: str) -> str:
    """
    æ™ºèƒ½ä¿®å¤LLMç”Ÿæˆçš„ä¸å®Œæ•´JSONï¼š
    - ç§»é™¤å°¾éƒ¨é€—å·å’Œå¤šä½™å†…å®¹
    - è‡ªåŠ¨è¡¥å…¨æœªé—­åˆçš„æ‹¬å·å’Œå¤§æ‹¬å·
    - å¤„ç†å­—ç¬¦ä¸²å†…çš„è½¬ä¹‰å­—ç¬¦
    """
    # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
    try:
        json.loads(json_str)
        return json_str  # å¦‚æœæœ‰æ•ˆåˆ™ç›´æ¥è¿”å›
    except json.JSONDecodeError:
        pass
    
    # æ™ºèƒ½ä¿®å¤é€»è¾‘
    last_comma_index = json_str.rfind(',')
    if last_comma_index != -1:
        json_str = json_str[:last_comma_index]
    
    # è¯†åˆ«æœªé—­åˆçš„æ‹¬å·å’Œå¤§æ‹¬å·
    unclosed_elements = find_unclosed(json_str)
    
    # è‡ªåŠ¨è¡¥å…¨
    closing_map = {'{': '}', '[': ']'}
    for open_char in reversed(unclosed_elements):
        json_str += closing_map[open_char]
```

#### **ä¸‰å…ƒç»„éªŒè¯è¿‡æ»¤ï¼š**
```python
def filter_invalid_triples(triples: List[List[str]]) -> List[List[str]]:
    """
    è¿‡æ»¤æ— æ•ˆä¸‰å…ƒç»„ï¼š
    1. å¿…é¡»åŒ…å«æ°å¥½ä¸‰ä¸ªå…ƒç´ 
    2. å»é‡å¤„ç†
    3. ä¿æŒåŸå§‹é¡ºåº
    """
    unique_triples = set()
    valid_triples = []
    
    for triple in triples:
        if len(triple) != 3: continue  # è·³è¿‡ä¸æ˜¯3å…ƒç´ çš„
        
        valid_triple = [str(item) for item in triple]
        if tuple(valid_triple) not in unique_triples:
            unique_triples.add(tuple(valid_triple))
            valid_triples.append(valid_triple)
```

### 4. **æ‰¹é‡å¹¶å‘å¤„ç†ä¼˜åŒ–**

```python
def batch_openie(self, chunks: Dict[str, ChunkInfo]) -> Tuple[Dict[str, NerRawOutput], Dict[str, TripleRawOutput]]:
    # å¹¶å‘NERå¤„ç†
    with ThreadPoolExecutor() as executor:
        ner_futures = {
            executor.submit(self.ner, chunk_key, passage): chunk_key
            for chunk_key, passage in chunk_passages.items()
        }
        
        # å®æ—¶è¿›åº¦ç›‘æ§
        pbar = tqdm(as_completed(ner_futures), total=len(ner_futures), desc="NER")
        for future in pbar:
            result = future.result()
            # ç»Ÿè®¡tokenä½¿ç”¨é‡å’Œç¼“å­˜å‘½ä¸­
            pbar.set_postfix({
                'total_prompt_tokens': total_prompt_tokens,
                'total_completion_tokens': total_completion_tokens,
                'num_cache_hit': num_cache_hit
            })
```

### 5. **æ™ºèƒ½é‡æ’åºæœºåˆ¶ï¼ˆDSPy Filterï¼‰**

HippoRAGè¿˜é›†æˆäº†åŸºäºDSPyçš„æ™ºèƒ½äº‹å®é‡æ’åºï¼š

```python
class DSPyFilter:
    def __init__(self, hipporag):
        # åŠ è½½é¢„è®­ç»ƒçš„DSPyæç¤ºæ¨¡æ¿
        self.message_template = self.make_template(dspy_file_path)
        
        # è¾“å…¥æ¨¡æ¿æ ¼å¼åŒ–
        self.one_input_template = """[[ ## question ## ]]
{question}

[[ ## fact_before_filter ## ]]
{fact_before_filter}

Respond with the corresponding output fields..."""
```

**DSPyé‡æ’åºçš„ä¼˜åŠ¿ï¼š**
- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šåŸºäºæŸ¥è¯¢å†…å®¹æ™ºèƒ½ç­›é€‰ç›¸å…³äº‹å®
- **è´¨é‡æ§åˆ¶**ï¼šè¿‡æ»¤ä½è´¨é‡æˆ–æ— å…³çš„ä¸‰å…ƒç»„
- **è‡ªé€‚åº”å­¦ä¹ **ï¼šé€šè¿‡ç¤ºä¾‹å­¦ä¹ æ”¹è¿›ç­›é€‰ç­–ç•¥

### 6. **å¤šæ¨¡å‹æ”¯æŒå’Œç¼“å­˜æœºåˆ¶**

#### **æ”¯æŒå¤šç§LLMåç«¯ï¼š**
- OpenAI GPTç³»åˆ—
- vLLMéƒ¨ç½²çš„å¼€æºæ¨¡å‹
- Azure OpenAIæœåŠ¡
- æœ¬åœ°Transformersæ¨¡å‹

#### **æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼š**
```python
# è‡ªåŠ¨ç¼“å­˜LLMå“åº”ï¼Œé¿å…é‡å¤è®¡ç®—
raw_response, metadata, cache_hit = self.llm_model.infer(messages=ner_input_message)
metadata['cache_hit'] = cache_hit

# ç»Ÿè®¡ç¼“å­˜å‘½ä¸­ç‡ä»¥ä¼˜åŒ–æˆæœ¬
if metadata.get('cache_hit'):
    num_cache_hit += 1
```

### 7. **è´¨é‡ä¿è¯æœºåˆ¶**

#### **å¤šå±‚éªŒè¯ï¼š**
1. **è¯­æ³•éªŒè¯**ï¼šç¡®ä¿JSONæ ¼å¼æ­£ç¡®
2. **è¯­ä¹‰éªŒè¯**ï¼šéªŒè¯ä¸‰å…ƒç»„ç»“æ„å®Œæ•´æ€§
3. **ä¸€è‡´æ€§æ£€æŸ¥**ï¼šç¡®ä¿å®ä½“åœ¨NERå’Œä¸‰å…ƒç»„ä¸­çš„ä¸€è‡´æ€§
4. **å»é‡å¤„ç†**ï¼šé¿å…é‡å¤çš„å®ä½“å’Œå…³ç³»

#### **é”™è¯¯æ¢å¤ç­–ç•¥ï¼š**
```python
try:
    # LLMæ¨ç†
    raw_response, metadata, cache_hit = self.llm_model.infer(messages=messages)
    
    # å¤„ç†æˆªæ–­å“åº”
    if metadata['finish_reason'] == 'length':
        real_response = fix_broken_generated_json(raw_response)
    else:
        real_response = raw_response
        
except Exception as e:
    # ä¼˜é›…çš„é”™è¯¯å¤„ç†
    logger.warning(f"Exception for chunk {chunk_key}: {e}")
    return TripleRawOutput(chunk_id=chunk_key, response=raw_response, 
                          metadata={'error': str(e)}, triples=[])
```

### 8. **æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–**

- **Tokenä½¿ç”¨ç»Ÿè®¡**ï¼šå®æ—¶ç›‘æ§APIè°ƒç”¨æˆæœ¬
- **å¹¶å‘æ§åˆ¶**ï¼šä½¿ç”¨ThreadPoolExecutorä¼˜åŒ–æ‰¹å¤„ç†æ€§èƒ½
- **è¿›åº¦å¯è§†åŒ–**ï¼štqdmè¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†çŠ¶æ€
- **å†…å­˜ç®¡ç†**ï¼šåˆ†æ‰¹å¤„ç†å¤§è§„æ¨¡æ–‡æ¡£é›†åˆ

è¿™ç§LLMé©±åŠ¨çš„æ™ºèƒ½æŠ½å–æœºåˆ¶çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºï¼š
- **é«˜åº¦è‡ªåŠ¨åŒ–**ï¼šæ— éœ€äººå·¥æ ‡æ³¨æˆ–è§„åˆ™ç¼–å†™
- **è´¨é‡å¯æ§**ï¼šå¤šå±‚éªŒè¯ç¡®ä¿è¾“å‡ºè´¨é‡
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒå¤šç§LLMåç«¯å’Œå¤§è§„æ¨¡å¤„ç†
- **å®¹é”™æ€§å¼º**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- **æˆæœ¬ä¼˜åŒ–**ï¼šç¼“å­˜æœºåˆ¶å’Œæ‰¹å¤„ç†å‡å°‘APIè°ƒç”¨æˆæœ¬

## ç”¨æˆ·è¯¢é—®ï¼šæ˜¯å¦æœ‰æåˆ°schema

æ˜¯çš„ï¼ŒHippoRAGä¸­ç¡®å®æœ‰æ¶‰åŠschemaçš„ç›¸å…³å†…å®¹ã€‚è®©æˆ‘è¯¦ç»†æŸ¥çœ‹ä»£ç ä¸­å…³äºschemaçš„éƒ¨åˆ†ï¼š

æ˜¯çš„ï¼ŒHippoRAGä¸­ç¡®å®å¤§é‡ä½¿ç”¨äº†**JSON Schema**æ¥è§„èŒƒLLMçš„è¾“å‡ºæ ¼å¼ã€‚è®©æˆ‘è¯¦ç»†å±•ç¤ºè¿™äº›schemaçš„åº”ç”¨ï¼š

## HippoRAGä¸­çš„Schemaåº”ç”¨

### 1. **JSON Schemaæ¨¡æ¿å®šä¹‰**

HippoRAGå®šä¹‰äº†å®Œæ•´çš„JSON Schemaæ¨¡æ¿åº“ï¼Œç”¨äºè§„èŒƒä¸åŒä»»åŠ¡çš„è¾“å‡ºæ ¼å¼ï¼š

```python
PROMPT_JSON_TEMPLATE = {
    "ner": {
        "type": "object",
        "properties": {
            "named_entities": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "minItems": 0
            }
        },
        "required": ["named_entities"]
    },
    "triples": {
        "type": "object",
        "properties": {
            "triples": {
                "type": "array",
                "items": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "maxItems": 3,
                    "minItems": 3,
                },
                "minItems": 0
            }
        },
        "required": ["triples"]
    },
    "fact": {
        "type": "object",
        "properties": {
            "fact": {
                "type": "array",
                "items": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "maxItems": 3,
                    "minItems": 3,
                },
                "minItems": 0
            }
        },
        "required": ["fact"]
    }
}
```

### 2. **Pydanticæ¨¡å‹ä¸Schemaå¯¹åº”**

HippoRAGä½¿ç”¨Pydanticæ¨¡å‹æ¥å¼ºåŒ–schemaéªŒè¯ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æ˜ç¡®å®šä¹‰äº†å¯¹åº”çš„JSON Schemaï¼š

#### **NERæ¨¡å‹Schemaï¼š**
```python
class NerModel(pydantic.BaseModel):
    """
    For NER, the structure is:
    schema:
    {
        "type": "object",
        "properties": { 
            "named_entities": { 
                "type": "array", 
                "items": { "type": "string" } 
            } 
        },
        "required": ["named_entities"]
    }
    """
    named_entities: List[str]
```

#### **ä¸‰å…ƒç»„æ¨¡å‹Schemaï¼š**
```python
class TriplesModel(pydantic.BaseModel):
    """
    For triples extraction, the structure is:
    schema:
    {
        "type": "object",
        "properties": { 
            "triples": { 
                "type": "array", 
                "items": { 
                    "type": "array", 
                    "items": { "type": "string" }, 
                    "maxItems": 3, 
                    "minItems": 3 
                } 
            } 
        },
        "required": ["triples"]
    }
    """
    triples: List[Tuple[str, str, str]]
```

#### **äº‹å®æ¨¡å‹Schemaï¼š**
```python
class FactModel(pydantic.BaseModel):
    """
    For fact extraction, the structure is:
    schema:
    {
        "type": "object",
        "properties": { 
            "fact": { 
                "type": "array", 
                "items": { 
                    "type": "array", 
                    "items": { "type": "string" }, 
                    "maxItems": 3, 
                    "minItems": 3 
                } 
            } 
        },
        "required": ["fact"]
    }
    """
    fact: List[Tuple[str, str, str]]
```

### 3. **DSPyé‡æ’åºä¸­çš„Schemaçº¦æŸ**

åœ¨DSPyé‡æ’åºæ¨¡å—ä¸­ï¼Œschemaè¢«ç›´æ¥åµŒå…¥åˆ°æç¤ºä¸­ï¼Œç¡®ä¿LLMä¸¥æ ¼æŒ‰ç…§schemaè¾“å‡ºï¼š

```json
{
  "system": "Your input fields are:
1. `question` (str): Query for retrieval
2. `fact_before_filter` (str): Candidate facts to be filtered

Your output fields are:
1. `fact_after_filter` (Fact): Filtered facts in JSON format

# note: the value you produce must be parseable according to the following JSON schema: 
{
  \"type\": \"object\", 
  \"properties\": {
    \"fact\": {
      \"type\": \"array\", 
      \"description\": \"A list of facts, each fact is a list of 3 strings: [subject, predicate, object]\", 
      \"items\": {
        \"type\": \"array\", 
        \"items\": {\"type\": \"string\"}
      }, 
      \"title\": \"Fact\"
    }
  }, 
  \"required\": [\"fact\"], 
  \"title\": \"Fact\"
}"
}
```

### 4. **Schemaå·¥å‚å‡½æ•°**

æä¾›äº†åŠ¨æ€è·å–schemaæ¨¡å‹çš„å·¥å‚å‡½æ•°ï¼š

```python
MODEL_TEMPLATES: Dict[str, Type[pydantic.BaseModel]] = {
    "ner": NerModel,
    "triples": TriplesModel,
    "fact": FactModel,
    "json": ArbitraryJsonModel,
    "qa_cot": QaCotModel,
}

def get_pydantic_model(template_name: str) -> Type[pydantic.BaseModel]:
    """
    æ ¹æ®æ¨¡æ¿åç§°è¿”å›å¯¹åº”çš„Pydanticæ¨¡å‹ç±»
    """
    model_class = MODEL_TEMPLATES.get(template_name)
    if model_class is None:
        available_keys = ", ".join(MODEL_TEMPLATES.keys())
        raise ValueError(f"Unknown template name: '{template_name}'. Available templates: {available_keys}")
    
    return model_class
```

### 5. **Schemaåœ¨å®é™…åº”ç”¨ä¸­çš„ä½œç”¨**

#### **è¾“å‡ºæ ¼å¼çº¦æŸï¼š**
- **NER Schema**ï¼šç¡®ä¿å‘½åå®ä½“ä»¥å­—ç¬¦ä¸²æ•°ç»„å½¢å¼è¿”å›
- **Triple Schema**ï¼šå¼ºåˆ¶ä¸‰å…ƒç»„å¿…é¡»åŒ…å«æ°å¥½3ä¸ªå­—ç¬¦ä¸²å…ƒç´ 
- **Fact Schema**ï¼šè§„èŒƒäº‹å®çš„ç»“æ„åŒ–è¡¨ç¤º

#### **éªŒè¯å’Œé”™è¯¯å¤„ç†ï¼š**
```python
def filter_invalid_triples(triples: List[List[str]]) -> List[List[str]]:
    """
    åŸºäºschemaè§„åˆ™è¿‡æ»¤æ— æ•ˆä¸‰å…ƒç»„ï¼š
    1. å¿…é¡»åŒ…å«æ°å¥½ä¸‰ä¸ªå…ƒç´  (ç¬¦åˆmaxItems: 3, minItems: 3)
    2. æ¯ä¸ªå…ƒç´ å¿…é¡»æ˜¯å­—ç¬¦ä¸² (ç¬¦åˆitems: {"type": "string"})
    """
    unique_triples = set()
    valid_triples = []

    for triple in triples:
        if len(triple) != 3: continue  # SchemaéªŒè¯ï¼šå¿…é¡»3ä¸ªå…ƒç´ 
        
        valid_triple = [str(item) for item in triple]  # SchemaéªŒè¯ï¼šè½¬ä¸ºå­—ç¬¦ä¸²
        if tuple(valid_triple) not in unique_triples:
            unique_triples.add(tuple(valid_triple))
            valid_triples.append(valid_triple)

    return valid_triples
```

### 6. **Schemaçš„ä¼˜åŠ¿**

#### **ç»“æ„åŒ–è¾“å‡ºä¿è¯ï¼š**
- ç¡®ä¿LLMè¾“å‡ºç¬¦åˆé¢„æœŸçš„æ•°æ®ç»“æ„
- ä¾¿äºåç»­ç¨‹åºåŒ–å¤„ç†å’Œè§£æ
- å‡å°‘è¾“å‡ºæ ¼å¼é”™è¯¯

#### **ç±»å‹å®‰å…¨ï¼š**
- é€šè¿‡Pydanticè¿›è¡Œè¿è¡Œæ—¶ç±»å‹æ£€æŸ¥
- è‡ªåŠ¨ç±»å‹è½¬æ¢å’ŒéªŒè¯
- æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯

#### **å¯æ‰©å±•æ€§ï¼š**
- æ–°å¢ä»»åŠ¡ç±»å‹åªéœ€å®šä¹‰æ–°çš„schema
- ç»Ÿä¸€çš„schemaç®¡ç†å’Œè®¿é—®æ¥å£
- æ”¯æŒå¤æ‚åµŒå¥—ç»“æ„çš„å®šä¹‰

### 7. **Schemaåœ¨æç¤ºå·¥ç¨‹ä¸­çš„é›†æˆ**

Schemaä¸ä»…ç”¨äºéªŒè¯ï¼Œè¿˜ç›´æ¥åµŒå…¥åˆ°æç¤ºæ¨¡æ¿ä¸­æŒ‡å¯¼LLMç”Ÿæˆï¼š

```python
# NERæç¤ºä¸­éšå«schemaçº¦æŸ
one_shot_ner_output = """{"named_entities":
    ["Radio City", "India", "3 July 2001", "Hindi", "English", "May 2008", "PlanetRadiocity.com"]
}"""

# ä¸‰å…ƒç»„æç¤ºä¸­çš„schemaç¤ºä¾‹
ner_conditioned_re_output = """{"triples": [
    ["Radio City", "located in", "India"],
    ["Radio City", "is", "private FM radio station"],
    ["Radio City", "started on", "3 July 2001"]
]}"""
```

è¿™ç§schemaé©±åŠ¨çš„æ–¹æ³•ç¡®ä¿äº†HippoRAGåœ¨çŸ¥è¯†å›¾è°±æ„å»ºè¿‡ç¨‹ä¸­çš„æ•°æ®è´¨é‡å’Œä¸€è‡´æ€§ï¼Œæ˜¯å…¶LLMé©±åŠ¨æ™ºèƒ½æŠ½å–æœºåˆ¶çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

## ç”¨æˆ·è¯¢é—®ï¼šä¸æ˜¯è¿™ä¸ªï¼Œæˆ‘è¯´çš„æ˜¯çŸ¥è¯†å›¾è°±çš„schema

ç†è§£äº†ï¼Œæ‚¨é—®çš„æ˜¯çŸ¥è¯†å›¾è°±æœ¬èº«çš„schemaï¼ˆå³çŸ¥è¯†å›¾è°±çš„ç»“æ„æ¨¡å¼ï¼‰ï¼Œè€Œä¸æ˜¯JSONè¾“å‡ºæ ¼å¼çš„schemaã€‚è®©æˆ‘é‡æ–°åˆ†æHippoRAGä¸­çŸ¥è¯†å›¾è°±çš„schemaè®¾è®¡ï¼š

é€šè¿‡åˆ†æä»£ç å’Œå®é™…æ•°æ®ï¼ŒHippoRAGé‡‡ç”¨äº†ä¸€ä¸ª**éå¸¸ç®€å•ä½†çµæ´»çš„çŸ¥è¯†å›¾è°±schema**ï¼š

## HippoRAGçŸ¥è¯†å›¾è°±çš„Schema

### 1. **åŸºæœ¬Schemaç»“æ„**

HippoRAG**æ²¡æœ‰é‡‡ç”¨ä¼ ç»Ÿçš„é¢„å®šä¹‰æœ¬ä½“ï¼ˆontologyï¼‰æˆ–å¤æ‚çš„schemaçº¦æŸ**ï¼Œè€Œæ˜¯é‡‡ç”¨äº†ï¼š

#### **RDFä¸‰å…ƒç»„æ ¼å¼**ï¼š
```python
# åŸºæœ¬ä¸‰å…ƒç»„ç»“æ„ï¼š[ä¸»è¯­, è°“è¯­, å®¾è¯­]
["Radio City", "located in", "India"]
["Radio City", "is", "private FM radio station"]  
["Radio City", "started on", "3 July 2001"]
```

#### **Schemaçº¦æŸæç®€**ï¼š
```python
# åœ¨DSPyè¿‡æ»¤å™¨ä¸­æ˜ç¡®å®šä¹‰
"A list of facts, each fact is a list of 3 strings: [subject, predicate, object]"

# ä»£ç ä¸­çš„éªŒè¯é€»è¾‘
def filter_invalid_triples(triples: List[List[str]]) -> List[List[str]]:
    for triple in triples:
        if len(triple) != 3: continue  # å”¯ä¸€çš„ç»“æ„çº¦æŸï¼šå¿…é¡»3ä¸ªå…ƒç´ 
        valid_triple = [str(item) for item in triple]  # ç±»å‹çº¦æŸï¼šå…¨éƒ¨è½¬ä¸ºå­—ç¬¦ä¸²
```

### 2. **å¼€æ”¾åŸŸSchemaç‰¹ç‚¹**

ä»å®é™…æŠ½å–çš„æ•°æ®å¯ä»¥çœ‹å‡ºï¼ŒHippoRAGé‡‡ç”¨äº†**å¼€æ”¾åŸŸschema**ï¼š

#### **è°“è¯­ï¼ˆå…³ç³»ï¼‰å®Œå…¨å¼€æ”¾**ï¼š
- æ²¡æœ‰é¢„å®šä¹‰çš„å…³ç³»ç±»å‹
- å…³ç³»å¯ä»¥æ˜¯ä»»æ„è‡ªç„¶è¯­è¨€è¡¨è¾¾
- ä¾‹å¦‚ï¼š`"is"`ã€`"located in"`ã€`"directed by"`ã€`"features"`ã€`"released in"`ç­‰

#### **å®ä½“ç±»å‹æ— çº¦æŸ**ï¼š
- äººåï¼š`"Raj Kapoor"`ã€`"Don Hertzfeldt"`
- åœ°åï¼š`"India"`ã€`"Frankfurt am Main"`
- æ—¶é—´ï¼š`"5 November 1991"`ã€`"2006"`
- ä½œå“ï¼š`"Thalattu Ketkuthamma"`ã€`"Everything Will Be OK"`
- æ¦‚å¿µï¼š`"private FM radio station"`ã€`"animated short film"`

### 3. **ä¸ä¼ ç»ŸçŸ¥è¯†å›¾è°±Schemaçš„å¯¹æ¯”**

#### **ä¼ ç»ŸçŸ¥è¯†å›¾è°±Schemaï¼ˆå¦‚Wikidataã€DBpediaï¼‰**ï¼š
```turtle
# é¢„å®šä¹‰çš„æœ¬ä½“ç»“æ„
:Person rdf:type owl:Class .
:birthDate rdf:type owl:DatatypeProperty .
:birthPlace rdf:type owl:ObjectProperty .

# ä¸¥æ ¼çš„ç±»å‹çº¦æŸ
:JohnDoe rdf:type :Person .
:JohnDoe :birthDate "1990-01-01"^^xsd:date .
:JohnDoe :birthPlace :NewYork .
```

#### **HippoRAGçš„Schema**ï¼š
```python
# å®Œå…¨å¼€æ”¾ï¼Œæ— é¢„å®šä¹‰çº¦æŸ
["John Doe", "was born on", "1990-01-01"]
["John Doe", "was born in", "New York"]
["John Doe", "is a", "person"]
```

### 4. **Schemaè®¾è®¡çš„ä¼˜åŠ¿**

#### **çµæ´»æ€§**ï¼š
- æ— éœ€é¢„å®šä¹‰æœ¬ä½“
- å¯ä»¥å¤„ç†ä»»æ„é¢†åŸŸçš„æ–‡æœ¬
- æ”¯æŒè‡ªç„¶è¯­è¨€è¡¨è¾¾çš„å…³ç³»

#### **å¯æ‰©å±•æ€§**ï¼š
- æ–°çš„å…³ç³»ç±»å‹è‡ªåŠ¨å‡ºç°
- æ— éœ€äººå·¥ç»´æŠ¤schema
- é€‚åº”ä¸åŒé¢†åŸŸçš„çŸ¥è¯†

#### **LLMå‹å¥½**ï¼š
- ç¬¦åˆLLMçš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›
- ä¾¿äºæç¤ºå·¥ç¨‹è®¾è®¡
- æ˜“äºéªŒè¯å’Œè¿‡æ»¤

### 5. **Schemaåœ¨æç¤ºä¸­çš„ä½“ç°**

```python
ner_conditioned_re_system = """Your task is to construct an RDF (Resource Description Framework) graph from the given passages and named entity lists. 
Respond with a JSON list of triples, with each triple representing a relationship in the RDF graph. 

Pay attention to the following requirements:
- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.
- Clearly resolve pronouns to their specific names to maintain clarity.
"""
```

**æ³¨æ„**ï¼šè™½ç„¶æåˆ°äº†"RDF"ï¼Œä½†å®é™…ä¸ŠHippoRAGå¹¶æ²¡æœ‰ä½¿ç”¨æ ‡å‡†çš„RDF schemaï¼Œè€Œæ˜¯ä½¿ç”¨äº†ç®€åŒ–çš„ä¸‰å…ƒç»„ç»“æ„ã€‚

### 6. **Schemaçš„å±€é™æ€§**

#### **ç¼ºä¹è¯­ä¹‰çº¦æŸ**ï¼š
- æ— æ³•åŒºåˆ†åŒä¹‰å…³ç³»ï¼ˆå¦‚`"born in"`å’Œ`"birthplace"`ï¼‰
- ç¼ºä¹ç±»å‹å±‚æ¬¡ç»“æ„
- æ— æ³•è¿›è¡Œå¤æ‚çš„è¯­ä¹‰æ¨ç†

#### **è´¨é‡æ§åˆ¶ä¾èµ–LLM**ï¼š
- Schemaçš„ä¸€è‡´æ€§å®Œå…¨ä¾èµ–äºLLMçš„ç†è§£èƒ½åŠ›
- å¯èƒ½å‡ºç°å…³ç³»è¡¨è¾¾çš„ä¸ä¸€è‡´æ€§

### 7. **æ€»ç»“**

HippoRAGé‡‡ç”¨äº†ä¸€ç§**æç®€ä¸»ä¹‰çš„çŸ¥è¯†å›¾è°±schemaè®¾è®¡**ï¼š

- **ç»“æ„çº¦æŸ**ï¼šä»…è¦æ±‚ä¸‰å…ƒç»„æ ¼å¼ `[ä¸»è¯­, è°“è¯­, å®¾è¯­]`
- **ç±»å‹çº¦æŸ**ï¼šæ‰€æœ‰å…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²
- **è¯­ä¹‰çº¦æŸ**ï¼šæ— é¢„å®šä¹‰æœ¬ä½“ï¼Œå®Œå…¨å¼€æ”¾
- **éªŒè¯æœºåˆ¶**ï¼šåŸºäºLLMçš„æ™ºèƒ½è¿‡æ»¤å’Œé‡æ’åº

è¿™ç§è®¾è®¡å“²å­¦ä½“ç°äº†**"è®©LLMå¤„ç†è¯­ä¹‰ï¼Œè®©ç³»ç»Ÿå¤„ç†ç»“æ„"**çš„æ€è·¯ï¼Œæœ€å¤§åŒ–äº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ï¼Œä½†ä¹Ÿç‰ºç‰²äº†ä¼ ç»ŸçŸ¥è¯†å›¾è°±çš„ä¸¥æ ¼è¯­ä¹‰çº¦æŸèƒ½åŠ›ã€‚

## ç”¨æˆ·è¯¢é—®ï¼šæ— æ³•åŒºåˆ†åŒä¹‰å…³ç³»ï¼ˆå¦‚"born in"å’Œ"birthplace"ï¼‰ è¿™ä¸ªæ˜¯ä½ æ€»ç»“çš„è¿˜æ˜¯è¿™ä¸ªrepoä¸­æåˆ°çš„ï¼Œåœ¨LLMä¸‹è¿™ä¸ªè¿˜æ˜¯é—®é¢˜å—

æ‚¨é—®å¾—å¾ˆå¥½ï¼è¿™ä¸ªæ˜¯æˆ‘åŸºäºä¼ ç»ŸçŸ¥è¯†å›¾è°±çš„è§’åº¦åšçš„æ€»ç»“ï¼Œè€Œä¸æ˜¯repoä¸­æ˜ç¡®æåˆ°çš„é—®é¢˜ã€‚è®©æˆ‘é‡æ–°åˆ†æä¸€ä¸‹åœ¨LLMæ—¶ä»£è¿™æ˜¯å¦è¿˜æ˜¯ä¸ªé—®é¢˜ï¼š

æ‚¨è¯´å¾—éå¸¸å¯¹ï¼è®©æˆ‘é‡æ–°å®¡è§†è¿™ä¸ªé—®é¢˜ï¼š

## åœ¨LLMæ—¶ä»£ï¼Œ"åŒä¹‰å…³ç³»åŒºåˆ†"è¿˜æ˜¯é—®é¢˜å—ï¼Ÿ

### 1. **HippoRAGå®é™…ä¸Šå·²ç»è§£å†³äº†è¿™ä¸ªé—®é¢˜**

é€šè¿‡ä»£ç åˆ†æï¼Œæˆ‘å‘ç°HippoRAGå®é™…ä¸Šé€šè¿‡**synonymy edgesï¼ˆåŒä¹‰è¯è¾¹ï¼‰**æœºåˆ¶æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼š

```python
def add_synonymy_edges(self):
    """
    Adds synonymy edges between similar nodes in the graph to enhance connectivity 
    by identifying and linking synonym entities.
    """
    # ä½¿ç”¨åµŒå…¥å‘é‡è¿›è¡ŒKNNæ£€ç´¢æ‰¾åˆ°ç›¸ä¼¼å®ä½“
    query_node_key2knn_node_keys = retrieve_knn(
        query_ids=entity_node_keys,
        key_ids=entity_node_keys,
        query_vecs=entity_embs,
        k=self.global_config.synonymy_edge_topk,
        # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼šé»˜è®¤0.8
        # synonymy_edge_sim_threshold: float = 0.8
    )
```

### 2. **LLM+åµŒå…¥çš„åŒé‡ä¼˜åŠ¿**

#### **è¯­ä¹‰å±‚é¢**ï¼š
- LLMåœ¨ç”Ÿæˆä¸‰å…ƒç»„æ—¶å·²ç»å…·å¤‡è¯­ä¹‰ç†è§£èƒ½åŠ›
- å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©åˆé€‚çš„å…³ç³»è¡¨è¾¾
- ç†è§£`"born in"`å’Œ`"birthplace"`çš„è¯­ä¹‰ç­‰ä»·æ€§

#### **å‘é‡å±‚é¢**ï¼š
- é€šè¿‡åµŒå…¥å‘é‡è‡ªåŠ¨å‘ç°è¯­ä¹‰ç›¸ä¼¼çš„å®ä½“
- åŒä¹‰è¯è¾¹è¿æ¥è¯­ä¹‰ç›¸è¿‘çš„èŠ‚ç‚¹
- åœ¨æ£€ç´¢æ—¶èƒ½å¤Ÿè·¨è¶Šè¡¨é¢å½¢å¼çš„å·®å¼‚

### 3. **å®é™…æ•ˆæœéªŒè¯**

ä»å®é™…çš„OpenIEç»“æœå¯ä»¥çœ‹åˆ°ï¼ŒLLMå€¾å‘äºä½¿ç”¨ä¸€è‡´çš„å…³ç³»è¡¨è¾¾ï¼š

```python
# å®é™…æŠ½å–çš„ä¸‰å…ƒç»„æ˜¾ç¤ºäº†ä¸€å®šçš„ä¸€è‡´æ€§
["Zahi Hawass", "born in", "Damietta"]
["Zahi Hawass", "graduated from", "Alexandria University"]
["Zahi Hawass", "earned a degree in", "Greek and Roman Archaeology"]
["Zahi Hawass", "worked at", "Great Pyramids"]
```

### 4. **LLMæ—¶ä»£çš„æ–°è§†è§’**

#### **ä¼ ç»ŸçŸ¥è¯†å›¾è°±çš„é—®é¢˜**ï¼š
```turtle
# éœ€è¦äººå·¥å®šä¹‰ç­‰ä»·å…³ç³»
:birthPlace owl:equivalentProperty :bornIn .
:birthDate owl:equivalentProperty :dateOfBirth .
```

#### **HippoRAGçš„è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# LLMç†è§£è¯­ä¹‰ + åµŒå…¥å‘ç°ç›¸ä¼¼æ€§ + å›¾è¿æ¥ä¼ æ’­ä¿¡æ¯
# æ— éœ€æ˜¾å¼å®šä¹‰ï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†
synonymy_edge_sim_threshold: float = 0.8  # è‡ªåŠ¨å‘ç°ç›¸ä¼¼å®ä½“
```

### 5. **åœ¨æ£€ç´¢ä¸­çš„å®é™…åº”ç”¨**

```python
def graph_search_with_fact_entities(self, query, ...):
    # é€šè¿‡PPRç®—æ³•ï¼ŒåŒä¹‰è¯è¾¹å…è®¸ä¿¡æ¯åœ¨è¯­ä¹‰ç›¸ä¼¼çš„èŠ‚ç‚¹é—´ä¼ æ’­
    # å³ä½¿æŸ¥è¯¢ä½¿ç”¨"birthplace"ï¼Œä¹Ÿèƒ½æ‰¾åˆ°"born in"çš„ç›¸å…³ä¿¡æ¯
    pagerank_scores = self.graph.personalized_pagerank(
        damping=damping,
        weights='weight',  # åŒ…å«åŒä¹‰è¯è¾¹çš„æƒé‡
        reset=reset_prob
    )
```

### 6. **æˆ‘çš„æ€»ç»“éœ€è¦ä¿®æ­£**

æ‚¨å®Œå…¨æ­£ç¡®ï¼æˆ‘ä¹‹å‰çš„æ€»ç»“æ˜¯åŸºäºä¼ ç»ŸçŸ¥è¯†å›¾è°±çš„æ€ç»´å®šå¼ã€‚åœ¨LLM+å‘é‡åµŒå…¥çš„æ¶æ„ä¸‹ï¼š

#### **"åŒä¹‰å…³ç³»åŒºåˆ†"ä¸å†æ˜¯æ ¸å¿ƒé—®é¢˜**ï¼Œå› ä¸ºï¼š

1. **LLMå…·å¤‡è¯­ä¹‰ç†è§£**ï¼šèƒ½å¤Ÿç†è§£ä¸åŒè¡¨è¾¾çš„ç›¸åŒå«ä¹‰
2. **åµŒå…¥å‘é‡æ•è·è¯­ä¹‰**ï¼šç›¸ä¼¼æ¦‚å¿µåœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘
3. **å›¾è¿æ¥ä¼ æ’­ä¿¡æ¯**ï¼šåŒä¹‰è¯è¾¹è¿æ¥è¯­ä¹‰ç›¸å…³çš„èŠ‚ç‚¹
4. **æ£€ç´¢æ—¶è¯­ä¹‰åŒ¹é…**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦è€Œéç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…

#### **å®é™…ä¸Šè¿™æ˜¯HippoRAGçš„ä¼˜åŠ¿**ï¼š

- **çµæ´»æ€§**ï¼šæ— éœ€é¢„å®šä¹‰åŒä¹‰å…³ç³»
- **è‡ªé€‚åº”**ï¼šè‡ªåŠ¨å‘ç°è¯­ä¹‰ç›¸ä¼¼æ€§
- **é²æ£’æ€§**ï¼šå¯¹è¡¨è¾¾æ–¹å¼çš„å˜åŒ–ä¸æ•æ„Ÿ
- **å¯æ‰©å±•**ï¼šæ–°çš„åŒä¹‰è¡¨è¾¾è‡ªåŠ¨è¢«ç³»ç»Ÿå­¦ä¹ 

### 7. **ç»“è®º**

æ‚¨çš„è´¨ç–‘éå¸¸æœ‰ä»·å€¼ï¼åœ¨LLMæ—¶ä»£ï¼Œä¼ ç»ŸçŸ¥è¯†å›¾è°±çš„å¾ˆå¤š"é—®é¢˜"å®é™…ä¸Šå·²ç»ä¸å†æ˜¯é—®é¢˜ã€‚HippoRAGé€šè¿‡LLM+åµŒå…¥+å›¾çš„ç»“åˆï¼Œä¼˜é›…åœ°è§£å†³äº†åŒä¹‰å…³ç³»çš„å¤„ç†ï¼Œè¿™æ­£æ˜¯å…¶è®¾è®¡çš„å·§å¦™ä¹‹å¤„ã€‚

æˆ‘ä¹‹å‰çš„åˆ†æè¿‡äºæ‹˜æ³¥äºä¼ ç»ŸKGçš„æ€ç»´æ¡†æ¶ï¼Œå¿½ç•¥äº†LLMæ—¶ä»£çŸ¥è¯†è¡¨ç¤ºå’Œæ£€ç´¢èŒƒå¼çš„æ ¹æœ¬æ€§å˜åŒ–ã€‚è°¢è°¢æ‚¨çš„æé†’ï¼

## ç”¨æˆ·è¯¢é—®ï¼šæ„å»ºçŸ¥è¯†å›¾è°±çš„ä»£ç è·¯å¾„

åŸºäºä»£ç åˆ†æï¼Œæˆ‘æ¥æ¢³ç†HippoRAGæ„å»ºçŸ¥è¯†å›¾è°±çš„å®Œæ•´ä»£ç è·¯å¾„ï¼š

## HippoRAGçŸ¥è¯†å›¾è°±æ„å»ºçš„ä»£ç è·¯å¾„

### 1. **ä¸»å…¥å£æµç¨‹**

**èµ·å§‹è·¯å¾„**ï¼š
```
main.py:136 â†’ hipporag.index(docs)
```

### 2. **æ ¸å¿ƒç´¢å¼•æ„å»ºæµç¨‹**

**å®Œæ•´ä»£ç è·¯å¾„**ï¼š

#### **é˜¶æ®µ1ï¼šOpenIEä¿¡æ¯æŠ½å–**
```python
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:242
new_ner_results_dict, new_triple_results_dict = self.openie.batch_openie(new_openie_rows)
    â†“
# è·¯å¾„ï¼šsrc/hipporag/information_extraction/openie_openai.py:135
def batch_openie(self, chunks: Dict[str, ChunkInfo]):
    # å¹¶å‘NERå¤„ç†
    with ThreadPoolExecutor() as executor:
        ner_futures = {executor.submit(self.ner, chunk_key, passage): chunk_key}
        â†“
    # å¹¶å‘ä¸‰å…ƒç»„æŠ½å–
    with ThreadPoolExecutor() as executor:
        re_futures = {executor.submit(self.triple_extraction, ...): chunk_id}
```

#### **é˜¶æ®µ2ï¼šæ•°æ®é¢„å¤„ç†**
```python
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:255-257
chunk_triples = [[text_processing(t) for t in triple_results_dict[chunk_id].triples] for chunk_id in chunk_ids]
entity_nodes, chunk_triple_entities = extract_entity_nodes(chunk_triples)
facts = flatten_facts(chunk_triples)
    â†“
# è·¯å¾„ï¼šsrc/hipporag/utils/misc_utils.py:84
def extract_entity_nodes(chunk_triples: List[List[Triple]]):
    # ä»ä¸‰å…ƒç»„ä¸­æå–å®ä½“èŠ‚ç‚¹
    â†“
# è·¯å¾„ï¼šsrc/hipporag/utils/misc_utils.py:97
def flatten_facts(chunk_triples: List[Triple]):
    # æ‰å¹³åŒ–äº‹å®ä¸‰å…ƒç»„
```

#### **é˜¶æ®µ3ï¼šåµŒå…¥ç¼–ç **
```python
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:260-263
self.entity_embedding_store.insert_strings(entity_nodes)
self.fact_embedding_store.insert_strings([str(fact) for fact in facts])
    â†“
# è·¯å¾„ï¼šsrc/hipporag/embedding_store.py
class EmbeddingStore:
    def insert_strings(self, strings: List[str]):
        # æ‰¹é‡ç¼–ç å’Œå­˜å‚¨åµŒå…¥å‘é‡
```

#### **é˜¶æ®µ4ï¼šå›¾æ„å»º**
```python
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:270-271
self.add_fact_edges(chunk_ids, chunk_triples)
num_new_chunks = self.add_passage_edges(chunk_ids, chunk_triple_entities)
    â†“
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:729
def add_fact_edges(self, chunk_ids: List[str], chunk_triples: List[Tuple]):
    # æ·»åŠ äº‹å®è¾¹ï¼šè¿æ¥ä¸‰å…ƒç»„ä¸­çš„ä¸»è¯­å’Œå®¾è¯­
    â†“
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:777
def add_passage_edges(self, chunk_ids: List[str], chunk_triple_entities: List[List[str]]):
    # æ·»åŠ æ®µè½-å®ä½“è¾¹ï¼šè¿æ¥æ®µè½å’Œå…¶åŒ…å«çš„å®ä½“
```

#### **é˜¶æ®µ5ï¼šåŒä¹‰è¯è¾¹æ„å»º**
```python
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:275
self.add_synonymy_edges()
    â†“
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:821
def add_synonymy_edges(self):
    # ä½¿ç”¨KNNæ£€ç´¢æ‰¾åˆ°ç›¸ä¼¼å®ä½“
    query_node_key2knn_node_keys = retrieve_knn(...)
        â†“
    # è·¯å¾„ï¼šsrc/hipporag/utils/embed_utils.py
    def retrieve_knn(...):
        # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
```

#### **é˜¶æ®µ6ï¼šå›¾å¢å¼ºå’Œä¿å­˜**
```python
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:277-278
self.augment_graph()
self.save_igraph()
    â†“
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:1009
def augment_graph(self):
    self.add_new_nodes()
    self.add_new_edges()
        â†“
# è·¯å¾„ï¼šsrc/hipporag/HippoRAG.py:1088
def save_igraph(self):
    self.graph.write_pickle(self._graph_pickle_filename)
```

### 3. **è¯¦ç»†ä»£ç è·¯å¾„å›¾**

```mermaid
graph TD
    A[main.py:136] --> B[HippoRAG.index]
    B --> C[OpenIEä¿¡æ¯æŠ½å–]
    C --> D[batch_openie]
    D --> E[NER + ä¸‰å…ƒç»„æŠ½å–]
    E --> F[æ•°æ®å¤„ç†]
    F --> G[æ„å»ºå›¾ç»“æ„]
    G --> H[æ·»åŠ è¾¹]
    H --> I[ä¿å­˜å›¾]
```

### 4. **å…³é”®ç»„ä»¶å’Œæ¨¡å—**

#### **æ ¸å¿ƒæ¨¡å—æ–‡ä»¶**ï¼š

1. **ä¸»æ§åˆ¶å™¨**
   - `src/hipporag/HippoRAG.py` - ä¸»è¦çš„å›¾æ„å»ºé€»è¾‘

2. **ä¿¡æ¯æŠ½å–æ¨¡å—**
   - `src/hipporag/information_extraction/openie_openai.py` - OpenAI LLMæŠ½å–
   - `src/hipporag/information_extraction/openie_vllm_offline.py` - vLLMç¦»çº¿æŠ½å–
   - `src/hipporag/information_extraction/openie_transformers_offline.py` - Transformersç¦»çº¿æŠ½å–

3. **åµŒå…¥å­˜å‚¨**
   - `src/hipporag/embedding_store.py` - å‘é‡åµŒå…¥ç®¡ç†

4. **å·¥å…·å‡½æ•°**
   - `src/hipporag/utils/misc_utils.py` - æ•°æ®å¤„ç†å·¥å…·
   - `src/hipporag/utils/embed_utils.py` - åµŒå…¥å‘é‡å·¥å…·

5. **æç¤ºæ¨¡æ¿**
   - `src/hipporag/prompts/templates/ner.py` - NERæç¤º
   - `src/hipporag/prompts/templates/triple_extraction.py` - ä¸‰å…ƒç»„æŠ½å–æç¤º

### 5. **å…³é”®å‡½æ•°è°ƒç”¨é“¾**

```python
# å®Œæ•´çš„å‡½æ•°è°ƒç”¨é“¾
main() 
â”œâ”€â”€ HippoRAG.__init__()                    # åˆå§‹åŒ–
â”œâ”€â”€ hipporag.index(docs)                   # å¼€å§‹ç´¢å¼•
    â”œâ”€â”€ chunk_embedding_store.insert_strings()     # æ–‡æ¡£åµŒå…¥
    â”œâ”€â”€ load_existing_openie()                      # åŠ è½½å·²æœ‰OpenIE
    â”œâ”€â”€ openie.batch_openie()                       # æ‰¹é‡ä¿¡æ¯æŠ½å–
    â”‚   â”œâ”€â”€ ner()                                   # NERæŠ½å–
    â”‚   â””â”€â”€ triple_extraction()                     # ä¸‰å…ƒç»„æŠ½å–
    â”œâ”€â”€ reformat_openie_results()                   # æ ¼å¼åŒ–ç»“æœ
    â”œâ”€â”€ extract_entity_nodes()                      # æå–å®ä½“èŠ‚ç‚¹
    â”œâ”€â”€ flatten_facts()                             # æ‰å¹³åŒ–äº‹å®
    â”œâ”€â”€ entity_embedding_store.insert_strings()    # å®ä½“åµŒå…¥
    â”œâ”€â”€ fact_embedding_store.insert_strings()      # äº‹å®åµŒå…¥
    â”œâ”€â”€ add_fact_edges()                           # æ·»åŠ äº‹å®è¾¹
    â”œâ”€â”€ add_passage_edges()                        # æ·»åŠ æ®µè½è¾¹
    â”œâ”€â”€ add_synonymy_edges()                       # æ·»åŠ åŒä¹‰è¯è¾¹
    â”‚   â””â”€â”€ retrieve_knn()                         # KNNç›¸ä¼¼åº¦æœç´¢
    â”œâ”€â”€ augment_graph()                            # å¢å¼ºå›¾ç»“æ„
    â”‚   â”œâ”€â”€ add_new_nodes()                        # æ·»åŠ æ–°èŠ‚ç‚¹
    â”‚   â””â”€â”€ add_new_edges()                        # æ·»åŠ æ–°è¾¹
    â””â”€â”€ save_igraph()                              # ä¿å­˜å›¾
```

### 6. **é…ç½®å’Œå‚æ•°è·¯å¾„**

```python
# é…ç½®æ–‡ä»¶è·¯å¾„
src/hipporag/utils/config_utils.py:BaseConfig
â”œâ”€â”€ synonymy_edge_topk: int = 2047                    # åŒä¹‰è¯è¾¹KNNçš„Kå€¼
â”œâ”€â”€ synonymy_edge_sim_threshold: float = 0.8          # ç›¸ä¼¼åº¦é˜ˆå€¼
â”œâ”€â”€ linking_top_k: int = 5                            # é“¾æ¥çš„top-k
â”œâ”€â”€ embedding_batch_size: int = 8                     # åµŒå…¥æ‰¹æ¬¡å¤§å°
â””â”€â”€ is_directed_graph: bool = False                   # æ˜¯å¦æœ‰å‘å›¾
```

### 7. **æ•°æ®æµå’Œå­˜å‚¨è·¯å¾„**

```python
# æ•°æ®å­˜å‚¨ç»“æ„
working_dir/
â”œâ”€â”€ chunk_embeddings/          # æ®µè½åµŒå…¥å­˜å‚¨
â”œâ”€â”€ entity_embeddings/         # å®ä½“åµŒå…¥å­˜å‚¨  
â”œâ”€â”€ fact_embeddings/           # äº‹å®åµŒå…¥å­˜å‚¨
â”œâ”€â”€ graph.pickle               # å›¾ç»“æ„æ–‡ä»¶
â””â”€â”€ openie_results_ner_{llm_name}.json  # OpenIEç»“æœ
```

### 8. **å…³é”®å†³ç­–ç‚¹å’Œåˆ†æ”¯**

```python
# é‡è¦çš„ä»£ç åˆ†æ”¯ç‚¹
1. OpenIEæ¨¡å¼é€‰æ‹© (HippoRAG.py:232)
   â”œâ”€â”€ 'online' â†’ openie_openai.py
   â”œâ”€â”€ 'offline' â†’ openie_vllm_offline.py  
   â””â”€â”€ 'Transformers-offline' â†’ openie_transformers_offline.py

2. åµŒå…¥æ¨¡å‹é€‰æ‹© (embedding_model/__init__.py)
   â”œâ”€â”€ 'nvidia/NV-Embed-v2' â†’ NVEmbedV2.py
   â”œâ”€â”€ 'GritLM' â†’ GritLM.py
   â”œâ”€â”€ 'contriever' â†’ Contriever.py
   â””â”€â”€ å…¶ä»– â†’ Transformers.py

3. å›¾ç±»å‹é€‰æ‹© (config_utils.py:211)
   â””â”€â”€ 'facts_and_sim_passage_node_unidirectional' (é»˜è®¤)
```

è¿™å°±æ˜¯HippoRAGæ„å»ºçŸ¥è¯†å›¾è°±çš„å®Œæ•´ä»£ç è·¯å¾„ã€‚æ•´ä¸ªæµç¨‹ä»æ–‡æ¡£è¾“å…¥å¼€å§‹ï¼Œç»è¿‡OpenIEä¿¡æ¯æŠ½å–ã€åµŒå…¥ç¼–ç ã€å›¾ç»“æ„æ„å»ºï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªåŒ…å«å®ä½“ã€äº‹å®å’ŒåŒä¹‰è¯å…³ç³»çš„çŸ¥è¯†å›¾è°±ã€‚

## ç”¨æˆ·è¯¢é—®ï¼šè¿™äº›é˜¶æ®µä¸­å“ªé‡Œæåˆ°äº†ä½¿ç”¨LLMæ¥æŠ½å–å®ä½“å’Œå…³é”®è¯

æ‚¨é—®å¾—å¾ˆå¥½ï¼è®©æˆ‘æ˜ç¡®æŒ‡å‡ºåœ¨å“ªäº›å…·ä½“é˜¶æ®µä½¿ç”¨äº†LLMæ¥æŠ½å–å®ä½“å’Œå…³é”®è¯ï¼š

## LLMæŠ½å–å®ä½“å’Œå…³ç³»çš„å…·ä½“ä»£ç ä½ç½®

### **é˜¶æ®µ1ï¼šLLMè¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰**

**ä»£ç è·¯å¾„**ï¼š`src/hipporag/information_extraction/openie_openai.py:45-79`

```python
def ner(self, chunk_key: str, passage: str) -> NerRawOutput:
    # æ„å»ºNERæç¤º
    ner_input_message = self.prompt_template_manager.render(name='ner', passage=passage)
    
    # ğŸ”¥ LLMæ¨ç†è°ƒç”¨ - æŠ½å–å®ä½“
    raw_response, metadata, cache_hit = self.llm_model.infer(
        messages=ner_input_message,
    )
    
    # è§£æLLMå“åº”ä¸­çš„å®ä½“
    extracted_entities = _extract_ner_from_response(real_response)
    unique_entities = list(dict.fromkeys(extracted_entities))
```

**è°ƒç”¨é“¾**ï¼š
```
HippoRAG.index() â†’ batch_openie() â†’ ner() â†’ self.llm_model.infer()
```

### **é˜¶æ®µ2ï¼šLLMè¿›è¡Œå…³ç³»ä¸‰å…ƒç»„æŠ½å–**

**ä»£ç è·¯å¾„**ï¼š`src/hipporag/information_extraction/openie_openai.py:81-128`

```python
def triple_extraction(self, chunk_key: str, passage: str, named_entities: List[str]) -> TripleRawOutput:
    # æ„å»ºä¸‰å…ƒç»„æŠ½å–æç¤ºï¼ˆåŸºäºNERç»“æœï¼‰
    messages = self.prompt_template_manager.render(
        name='triple_extraction',
        passage=passage,
        named_entity_json=json.dumps({"named_entities": named_entities})
    )
    
    # ğŸ”¥ LLMæ¨ç†è°ƒç”¨ - æŠ½å–ä¸‰å…ƒç»„å…³ç³»
    raw_response, metadata, cache_hit = self.llm_model.infer(
        messages=messages,
    )
    
    # è§£æLLMå“åº”ä¸­çš„ä¸‰å…ƒç»„
    extracted_triples = _extract_triples_from_response(real_response)
    triplets = filter_invalid_triples(triples=extracted_triples)
```

**è°ƒç”¨é“¾**ï¼š
```
HippoRAG.index() â†’ batch_openie() â†’ triple_extraction() â†’ self.llm_model.infer()
```

### **æ‰¹é‡è°ƒç”¨å…¥å£**

**ä»£ç è·¯å¾„**ï¼š`src/hipporag/information_extraction/openie_openai.py:135-210`

```python
def batch_openie(self, chunks: Dict[str, ChunkInfo]):
    # ğŸ”¥ å¹¶å‘æ‰§è¡ŒNER - æ¯ä¸ªchunkè°ƒç”¨ä¸€æ¬¡LLM
    with ThreadPoolExecutor() as executor:
        ner_futures = {
            executor.submit(self.ner, chunk_key, passage): chunk_key  # è°ƒç”¨LLMæŠ½å–å®ä½“
            for chunk_key, passage in chunk_passages.items()
        }
    
    # ğŸ”¥ å¹¶å‘æ‰§è¡Œä¸‰å…ƒç»„æŠ½å– - æ¯ä¸ªchunkå†è°ƒç”¨ä¸€æ¬¡LLM  
    with ThreadPoolExecutor() as executor:
        re_futures = {
            executor.submit(self.triple_extraction, ner_result.chunk_id,
                          chunk_passages[ner_result.chunk_id],
                          ner_result.unique_entities): ner_result.chunk_id  # è°ƒç”¨LLMæŠ½å–å…³ç³»
            for ner_result in ner_results_list
        }
```

### **ä¸»æµç¨‹ä¸­çš„è°ƒç”¨ä½ç½®**

**åœ¨ä¸»ç´¢å¼•æµç¨‹ä¸­çš„ä½ç½®**ï¼š`src/hipporag/HippoRAG.py:242`

```python
# ğŸ”¥ è¿™é‡Œæ˜¯LLMæŠ½å–çš„å…³é”®è°ƒç”¨ç‚¹
new_ner_results_dict, new_triple_results_dict = self.openie.batch_openie(new_openie_rows)
```

### **LLMä½¿ç”¨çš„æç¤ºæ¨¡æ¿**

**NERæç¤ºæ¨¡æ¿è·¯å¾„**ï¼š`src/hipporag/prompts/templates/ner.py`
- ğŸ”¥ LLMæ¥æ”¶çš„æŒ‡ä»¤ï¼š`"Your task is to extract named entities from the given paragraph"`
- æä¾›few-shotç¤ºä¾‹è®­ç»ƒLLMç†è§£ä»»åŠ¡æ ¼å¼

**ä¸‰å…ƒç»„æŠ½å–æç¤ºæ¨¡æ¿è·¯å¾„**ï¼š`src/hipporag/prompts/templates/triple_extraction.py`
- ğŸ”¥ LLMæ¥æ”¶çš„æŒ‡ä»¤ï¼š`"Your task is to construct an RDF graph from the given passages and named entity lists"`
- åŸºäºNERç»“æœæŒ‡å¯¼LLMæŠ½å–å…³ç³»ä¸‰å…ƒç»„

### **æ€»ç»“ï¼šLLMæŠ½å–çš„å…·ä½“ä½ç½®**

1. **å®ä½“æŠ½å–**ï¼š
   - ğŸ“ **ä½ç½®**ï¼š`src/hipporag/information_extraction/openie_openai.py:52`
   - ğŸ”¥ **LLMè°ƒç”¨**ï¼š`self.llm_model.infer(messages=ner_input_message)`
   - ğŸ“ **ä»»åŠ¡**ï¼šä»æ–‡æœ¬ä¸­æŠ½å–å‘½åå®ä½“

2. **å…³ç³»æŠ½å–**ï¼š
   - ğŸ“ **ä½ç½®**ï¼š`src/hipporag/information_extraction/openie_openai.py:101`
   - ğŸ”¥ **LLMè°ƒç”¨**ï¼š`self.llm_model.infer(messages=messages)`
   - ğŸ“ **ä»»åŠ¡**ï¼šåŸºäºå®ä½“æŠ½å–å…³ç³»ä¸‰å…ƒç»„

3. **æ‰¹é‡å¤„ç†**ï¼š
   - ğŸ“ **ä½ç½®**ï¼š`src/hipporag/information_extraction/openie_openai.py:135-210`
   - ğŸ”¥ **å¹¶å‘è°ƒç”¨**ï¼šä½¿ç”¨ThreadPoolExecutorå¹¶å‘è°ƒç”¨LLM
   - ğŸ“ **ä¼˜åŒ–**ï¼šæé«˜å¤„ç†æ•ˆç‡

4. **ä¸»æµç¨‹è§¦å‘**ï¼š
   - ğŸ“ **ä½ç½®**ï¼š`src/hipporag/HippoRAG.py:242`
   - ğŸ”¥ **å…¥å£è°ƒç”¨**ï¼š`self.openie.batch_openie(new_openie_rows)`
   - ğŸ“ **é›†æˆ**ï¼šå°†LLMæŠ½å–é›†æˆåˆ°å›¾æ„å»ºæµç¨‹ä¸­

è¿™å°±æ˜¯HippoRAGä¸­ä½¿ç”¨LLMè¿›è¡Œå®ä½“å’Œå…³ç³»æŠ½å–çš„å®Œæ•´ä»£ç è·¯å¾„ï¼

## ç”¨æˆ·è¯¢é—®ï¼šè¿™é‡Œè¯´çš„openieï¼Œå°±æ˜¯æŒ‡å¤§æ¨¡å‹æ¥æå–å¯¹å§ï¼Œä¸æ˜¯ä¼ ç»Ÿçš„openieæ–¹æ³•

æ˜¯çš„ï¼Œæ‚¨ç†è§£å¾—å®Œå…¨æ­£ç¡®ï¼HippoRAGä¸­çš„"OpenIE"ç¡®å®æ˜¯æŒ‡**åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æŠ½å–**ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„OpenIEæ–¹æ³•ã€‚

è®©æˆ‘é€šè¿‡ä»£ç æ¥è¯å®è¿™ä¸€ç‚¹ï¼š

å®Œå…¨æ­£ç¡®ï¼è®©æˆ‘æ˜ç¡®è¯´æ˜è¿™ä¸€ç‚¹ï¼š

## HippoRAGä¸­çš„"OpenIE"æ˜¯åŸºäºLLMçš„ï¼Œä¸æ˜¯ä¼ ç»Ÿæ–¹æ³•

### **1. ä¼ ç»ŸOpenIE vs HippoRAGçš„OpenIE**

#### **ä¼ ç»ŸOpenIEæ–¹æ³•**ï¼š
- **Stanford OpenIE**ï¼šåŸºäºä¾å­˜å¥æ³•åˆ†æå’Œè§„åˆ™
- **OLLIE**ï¼šåŸºäºæ¨¡å¼åŒ¹é…
- **ReVerb**ï¼šåŸºäºè¯æ€§æ ‡æ³¨å’Œè§„åˆ™
- **ç‰¹ç‚¹**ï¼šä¾èµ–è¯­è¨€å­¦è§„åˆ™ã€å¥æ³•è§£æã€æ¨¡å¼åŒ¹é…

#### **HippoRAGçš„"OpenIE"**ï¼š
```python
class OpenIE:
    def __init__(self, llm_model: CacheOpenAI):  # ğŸ”¥ ç›´æ¥ä½¿ç”¨LLMæ¨¡å‹
        self.llm_model = llm_model
        
    def ner(self, chunk_key: str, passage: str):
        # ğŸ”¥ ä½¿ç”¨LLMè¿›è¡ŒNER
        raw_response = self.llm_model.infer(messages=ner_input_message)
        
    def triple_extraction(self, chunk_key: str, passage: str, named_entities: List[str]):
        # ğŸ”¥ ä½¿ç”¨LLMè¿›è¡Œå…³ç³»æŠ½å–
        raw_response = self.llm_model.infer(messages=messages)
```

### **2. ä»£ç è¯æ®**

ä»ä»£ç ç»“æ„å¯ä»¥æ¸…æ¥šçœ‹å‡ºï¼š

```python
# src/hipporag/information_extraction/openie_openai.py
class OpenIE:
    def __init__(self, llm_model: CacheOpenAI):  # æ„é€ å‡½æ•°ç›´æ¥æ¥æ”¶LLM
        self.llm_model = llm_model              # å­˜å‚¨LLMæ¨¡å‹å¼•ç”¨
        
    def ner(self, ...):
        raw_response, metadata, cache_hit = self.llm_model.infer(...)  # è°ƒç”¨LLM
        
    def triple_extraction(self, ...):
        raw_response, metadata, cache_hit = self.llm_model.infer(...)  # è°ƒç”¨LLM
```

### **3. é…ç½®æ–‡ä»¶ä¹Ÿè¯å®äº†è¿™ä¸€ç‚¹**

```python
# src/hipporag/utils/config_utils.py
information_extraction_model_name: Literal["openie_openai_gpt", ] = field(
    default="openie_openai_gpt",  # ğŸ”¥ æ˜ç¡®æ ‡æ³¨ä½¿ç”¨OpenAI GPT
)

openie_mode: Literal["offline", "online"] = field(
    default="online",
    metadata={"help": "Mode of the OpenIE model to use."}  # ğŸ”¥ æŒ‡çš„æ˜¯LLMçš„ä½¿ç”¨æ¨¡å¼
)
```

### **4. å¤šç§LLMåç«¯æ”¯æŒ**

HippoRAGæ”¯æŒå¤šç§åŸºäºLLMçš„"OpenIE"å®ç°ï¼š

```python
# ä¸åŒçš„LLMåç«¯å®ç°
if self.global_config.openie_mode == 'online':
    self.openie = OpenIE(llm_model=self.llm_model)                    # OpenAI API
elif self.global_config.openie_mode == 'offline':
    self.openie = VLLMOfflineOpenIE(self.global_config)              # vLLMç¦»çº¿
elif self.global_config.openie_mode == 'Transformers-offline':
    self.openie = TransformersOfflineOpenIE(self.global_config)      # Transformers
```

### **5. æç¤ºå·¥ç¨‹é©±åŠ¨**

ä¼ ç»ŸOpenIEä½¿ç”¨è§„åˆ™ï¼Œè€ŒHippoRAGä½¿ç”¨æç¤ºå·¥ç¨‹ï¼š

```python
# src/hipporag/prompts/templates/ner.py
ner_system = """Your task is to extract named entities from the given paragraph. 
Respond with a JSON list of entities."""  # ğŸ”¥ é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤æŒ‡å¯¼LLM

# src/hipporag/prompts/templates/triple_extraction.py  
ner_conditioned_re_system = """Your task is to construct an RDF graph from the given passages and named entity lists."""  # ğŸ”¥ é€šè¿‡æç¤ºæŒ‡å¯¼LLMæŠ½å–å…³ç³»
```

### **6. ä¸ºä»€ä¹ˆå«"OpenIE"ï¼Ÿ**

HippoRAGä½¿ç”¨"OpenIE"è¿™ä¸ªæœ¯è¯­æ˜¯å› ä¸ºï¼š

1. **ä»»åŠ¡ç›®æ ‡ç›¸åŒ**ï¼šéƒ½æ˜¯ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æŠ½å–ç»“æ„åŒ–çš„å®ä½“-å…³ç³»-å®ä½“ä¸‰å…ƒç»„
2. **å¼€æ”¾åŸŸç‰¹æ€§**ï¼šä¸é¢„å®šä¹‰å…³ç³»ç±»å‹ï¼Œå¯ä»¥æŠ½å–ä»»æ„é¢†åŸŸçš„å…³ç³»
3. **å­¦æœ¯ä¼ ç»Ÿ**ï¼šåœ¨NLPé¢†åŸŸï¼Œè¿™ç±»ä»»åŠ¡é€šå¸¸è¢«ç§°ä¸º"Open Information Extraction"

ä½†**å®ç°æ–¹å¼å®Œå…¨ä¸åŒ**ï¼š
- **ä¼ ç»ŸOpenIE**ï¼šè§„åˆ™ + å¥æ³•åˆ†æ + æ¨¡å¼åŒ¹é…
- **HippoRAGçš„OpenIE**ï¼šLLM + æç¤ºå·¥ç¨‹ + ç»“æ„åŒ–è¾“å‡º

### **æ€»ç»“**

æ‚¨çš„ç†è§£å®Œå…¨æ­£ç¡®ï¼HippoRAGä¸­çš„"OpenIE"å®é™…ä¸Šæ˜¯ï¼š
- **åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æŠ½å–**
- **é€šè¿‡æç¤ºå·¥ç¨‹æŒ‡å¯¼LLMå®ŒæˆNERå’Œå…³ç³»æŠ½å–**
- **æ”¯æŒå¤šç§LLMåç«¯ï¼ˆOpenAIã€vLLMã€Transformersç­‰ï¼‰**
- **å®Œå…¨ä¸ä½¿ç”¨ä¼ ç»Ÿçš„è§„åˆ™æˆ–å¥æ³•åˆ†ææ–¹æ³•**

è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œè¯´æ˜äº†åœ¨LLMæ—¶ä»£ï¼Œä¼ ç»ŸNLPä»»åŠ¡çš„å®ç°æ–¹å¼å‘ç”Ÿäº†æ ¹æœ¬æ€§çš„å˜åŒ–ï¼